{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES WITH SMS SPAM COLLECTION DATASET: A TEXT MINING CASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This exercise is adapted from [Chapter 4 of \"Machine Learning with R\" by Brett Lantz](https://books.google.com.tr/books?id=ZaJNCgAAQBAJ&printsec=frontcover&hl=tr&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false)\n",
    "\n",
    "- To develop the Naive Bayes classifier, we will use data adapted from the SMS Spam\n",
    "Collection at http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/.\n",
    "\n",
    "- This dataset includes the text of SMS messages along with a label indicating\n",
    "whether the message is unwanted. Junk messages are labeled spam, while\n",
    "legitimate messages are labeled ham. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to continue from a previously saved session state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionfile <- \"02_naive_bayes_01.RData\"\n",
    "\n",
    "if(file.exists(sessionfile)) load(sessionfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table) # to handle the data in a more convenient manner\n",
    "library(tidyverse) # for a better work flow and more tools to wrangle and visualize the data\n",
    "library(tm) # for text mining\n",
    "library(SnowballC) # for word stemming\n",
    "library(gridExtra) # for multiple plots\n",
    "library(wordcloud) # visualize text data\n",
    "library(RColorBrewer) # for beautifying visualizations with custom colors\n",
    "library(e1071) # for naive bayes\n",
    "library(gmodels) # model evaluation\n",
    "library(knitr) # for better table printing\n",
    "library(kableExtra) # for better table printing\n",
    "library(scales) # for formatting numbers\n",
    "library(magrittr) # tools for better handling data structures\n",
    "library(purrr) # tools for better handling data structures\n",
    "library(IRdisplay) # printing html tables from kable\n",
    "options(warn = -1) # for suppressing messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's read the data into a data.table object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_raw <- fread(\"../data/csv/02_01_sms_spam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(sms_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample 10 ham and 10 spam entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2018)\n",
    "sample1 <- data.table(ham = sms_raw[type == \"ham\"][sample(.N, 10), text],\n",
    "           spam = sms_raw[type == \"spam\"][sample(.N, 10), text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"free\" and \"urgent\" words appear in spams while not in hams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the structure of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better that we convert \"type\" from character to factor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_raw[,type := as.factor(type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Standardize Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMS messages are strings of text composed of words, spaces, numbers, and\n",
    "punctuation.\n",
    "\n",
    "Handling this type of complex data takes a lot of thought and\n",
    "effort.\n",
    "\n",
    "One needs to consider how to remove numbers and punctuation; handle\n",
    "uninteresting words such as and, but, and or; and how to break apart sentences into\n",
    "individual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in processing text data involves creating a corpus, which is a collection of text documents.\n",
    "\n",
    "The documents can be short or long, from individual news articles, pages in a book or on the web, or entire books.\n",
    "\n",
    "In our case, the corpus will be a collection of SMS messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text with VectorSource and create corpus with VCorpus\n",
    "\n",
    "sms_corpus <- sms_raw[,tm::VectorSource(text)] %>% tm::VCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus holds documents for each of the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a summary of specific messages with tm::inspect() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm::inspect(sms_corpus[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual message, we should convert a list item to character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.character(sms_corpus[[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For viewing multiple messages, we'll use sapply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(sms_corpus[1:2], as.character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform our analysis, we need to divide these messages into individual words.\n",
    "\n",
    "But first, we need to clean the text, in order to standardize the words, by removing punctuation and other characters that clutter the result.\n",
    "\n",
    "For example, we would like the strings Hello!, HELLO, and hello to be counted as instances of the same word.\n",
    "\n",
    "The tm_map() function provides a method to apply a transformation (also known as mapping) to a tm corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tm_map to a corpus object is what \"lapply\" to an ordinary list object is: It applies the same function to all of its items and returns a corpus object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case lowering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first order of business will be to standardize the messages to use only lowercase\n",
    "characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_clean <- tm::tm_map(sms_corpus, tm::content_transformer(tolower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare a message before and after transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as.character(sms_corpus[[1]])\n",
    "as.character(sms_corpus_clean[[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove numbers from messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_clean <- tm::tm_map(sms_corpus_clean, removeNumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sapply(sms_corpus[4:5], as.character)\n",
    "sapply(sms_corpus_clean[4:5], as.character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next task is to remove filler words such as to, and, but, and or from our SMS\n",
    "messages.\n",
    "\n",
    "These terms are known as stop words and are typically removed prior to\n",
    "text mining.\n",
    "\n",
    "This is due to the fact that although they appear very frequently, they do\n",
    "not provide much useful information for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm::stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_clean <- tm::tm_map(sms_corpus_clean, tm::removeWords, tm::stopwords())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also eliminate any punctuation from\n",
    "the text messages using the built-in removePunctuation() transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_clean <- tm::tm_map(sms_corpus_clean, tm::removePunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also write a custom function to replace punctuation with whitespaces instead of removing them and then apply with tm_map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacePunctuation <- function(x)\n",
    "{\n",
    "    gsub(\"[[:punct:]]+\", \" \", x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removePunctuation(\"Hello World\")\n",
    "replacePunctuation(\"Hello...World\")\n",
    "replacePunctuation(\"Hello... World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common standardization for text data involves reducing words to their root form in a process called stemming.\n",
    "\n",
    "The stemming process takes words like learned,learning, and learns, and strips the suffix in order to transform them into the base\n",
    "form, learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows machine learning algorithms to treat the related terms as a single concept rather than attempting to learn a pattern for each variant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example on how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SnowballC::wordStem(c(\"learn\", \"learned\", \"learning\", \"learns\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We \"apply\" this function to a corpus through the tm function tm::stemDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_clean <- tm::tm_map(sms_corpus_clean, tm::stemDocument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed = 1500\n",
    "samplerows <- sample(1:length(sms_corpus), 10)\n",
    "data.frame(type = sms_raw[samplerows, type],\n",
    "           original = sapply(sms_corpus[samplerows], as.character),\n",
    "          cleaned = sapply(sms_corpus_clean[samplerows], as.character))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should strip additional whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm::stripWhitespace(\"a       a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_corpus_clean <- tm::tm_map(sms_corpus_clean, tm::stripWhitespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data are processed to our liking, the final step is to split the messages\n",
    "into individual components through a process called tokenization.\n",
    "\n",
    "A token is a single element of a text string; in this case, the tokens are words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two options two have an object for this:\n",
    "\n",
    "- a data structure called a Document Term Matrix (DTM) in which rows indicate documents (SMS messages) and columns indicate terms (words).\n",
    "\n",
    "- a data structure for a Term Document Matrix (TDM), which is simply a transposed DTM in which the rows are terms and the columns are documents.\n",
    "\n",
    "\n",
    "Why the need for both?\n",
    "\n",
    "Sometimes, it is more convenient to work with one or the other.\n",
    "\n",
    "- For example, if the number of documents is small, while the word list is large, it may make sense to use a TDM because it is generally easier to display many rows than to display many columns.\n",
    "- This said, the two are often interchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the DocumentTermMatrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dtm <- tm::DocumentTermMatrix(sms_corpus_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the structure and an excerpt of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should interpret above object as follows:\n",
    "\n",
    "- There are a total of 5559 documents and 6559 terms\n",
    "- Although there are 3.6e7 possible doc-term matches (5559 * 6559), each document contains only a handful of terms. Total number of doc-term matches are 42147 (where a document has at least one instance of a term)\n",
    "- \"i\" object shows the index of the docs in 42K matches\n",
    "- \"j\" object shows the index of the terms in 42K matches\n",
    "- \"v\" object shows the count of the appearance of the term in the match\n",
    "\n",
    "- First document has one instances of 967., 2282., 2581. 2938. and 6210. terms each\n",
    "\n",
    "Let's check:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the 1st document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doci <- 1\n",
    "doci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's view the first document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(sms_corpus_clean[doci], as.character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term matches of the 1st document occurs in the dtm at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices in dtm where matches of 1st document occurs \n",
    "dtm_indices_1 <- which(sms_dtm$i == doci)\n",
    "dtm_indices_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the term indices of those matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_indices <- sms_dtm$j[dtm_indices_1]\n",
    "term_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See those terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dtm$dimnames$Terms[term_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just the sorted and unique versions of the terms of the 1st document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, v shows the count of occurences of the term inside the doc. Let's possible values in this corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vs <- unique(sms_dtm$v)\n",
    "unique_vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is at least one instance in which a term appears 15 times in a doc. Let's get that:\n",
    "\n",
    "First let's find the indices of 15 occurences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_at_max_v <- which(sms_dtm$v == max(unique_vs))\n",
    "index_at_max_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See which term it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ind_at_max_v <- sms_dtm$j[index_at_max_v]\n",
    "term_ind_at_max_v\n",
    "\n",
    "sms_dtm$dimnames$Terms[term_ind_at_max_v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see which doc it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ind_at_max_v <- sms_dtm$i[index_at_max_v]\n",
    "doc_ind_at_max_v\n",
    "\n",
    "sapply(sms_corpus_clean[doc_ind_at_max_v], as.character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also index the dtm as a matrix and view the contents with inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(sms_dtm[doc_ind_at_max_v, term_ind_at_max_v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get 10 random matches (docs + terms) and subset the dtm for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2000)\n",
    "sample2 <- sample(length(sms_dtm[[1]]), 10)\n",
    "\n",
    "docs <- sms_dtm[[1]][sample2]\n",
    "terms <- sms_dtm[[2]][sample2]\n",
    "\n",
    "sample_mat <- tm::inspect(sms_dtm[docs, terms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it would be better to view them together with the doc contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.table(docs = sapply(sms_corpus_clean[docs], as.character), sample_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created an sms_dtm object that contains the tokenized corpus using the default settings, which apply minimal processing.\n",
    "\n",
    "The default settings are appropriate because we have already prepared the corpus manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we hadn't performed the preprocessing, we could do so\n",
    "here by providing a list of control parameter options to override the defaults.\n",
    "\n",
    "For example, to create a DTM directly from the raw, unprocessed SMS corpus, we can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dtm2 <- DocumentTermMatrix(sms_corpus,\n",
    "    control = list(\n",
    "        tolower = TRUE,\n",
    "        removeNumbers = TRUE,\n",
    "        stopwords = TRUE,\n",
    "        removePunctuation = TRUE,\n",
    "        stemming = TRUE\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_dtm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might have some differences with the previous dtm due to application order of cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_tdm <- tm::TermDocumentMatrix(sms_corpus_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a .75/.25 split, we will have 4169 train and 1390 test observations:\n",
    "\n",
    "Remember that, sms_raw is a data.table and in a data.table, .I is a placeholder for 1:nrows(DTobject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind <- 1:4169\n",
    "test_ind <- sms_raw[,.I[-train_ind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dtm_train <- sms_dtm[train_ind,]\n",
    "sms_dtm_test <- sms_dtm[test_ind,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a respective split of the type vector also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_train_labels <- sms_raw[train_ind, type]\n",
    "sms_test_labels <- sms_raw[test_ind, type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the subsets are representative of the complete set of SMS data, let's\n",
    "compare the proportion of spam in the training and test data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p1 <- ggplot2::ggplot(data.frame(labels = sms_train_labels)) +\n",
    "geom_bar(aes(x = labels, y = ..count../sum(..count..)), height = 0.1) +\n",
    "ggtitle(\"Train Labels\") +\n",
    "labs(x = \"type\", y = \"proportion\")\n",
    "\n",
    "p2 <- ggplot2::ggplot(data.frame(labels = sms_test_labels)) +\n",
    "geom_bar(aes(x = labels, y = ..count../sum(..count..)), height = 0.1) +\n",
    "ggtitle(\"Test Labels\") +\n",
    "labs(x = \"type\", y = \"proportion\")\n",
    "\n",
    "gridExtra::grid.arrange(p1, p2, ncol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportions are alike across sets.\n",
    "\n",
    "We can also confirm this with prop tables:\n",
    "\n",
    "Note the use of sapply against the newly created list with titles and the closure - unnamed embedded function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(list(Train = sms_train_labels,\n",
    "            Test = sms_test_labels),\n",
    "      function(x) prop.table(table(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize text data with word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word cloud is a way to visually depict the frequency at which words appear in text data.\n",
    "\n",
    "The cloud is composed of words scattered somewhat randomly around the figure.\n",
    "\n",
    "Words appearing more often in the text are shown in a larger font, while less common terms are shown in smaller fonts.\n",
    "\n",
    "This type of figures grew in popularity recently, since it provides a way to observe trending topics on social media websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a word cloud directly from the corpus, for words that appear at least 50 times.\n",
    "\n",
    "With random.order = F, more frequent words are placed closer to the center.\n",
    "\n",
    "We beautify the cloud with RColorBrewer package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud::wordcloud(sms_corpus_clean,\n",
    "                        min.freq = 50,\n",
    "                        random.order = F,\n",
    "                        colors = RColorBrewer::brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create separate word clouds for hams and spams directly from the raw sms object.\n",
    "\n",
    "wordcloud automatically does the necessary transformations\n",
    "\n",
    "We do that in the concise \"data.table\" way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# open new plot\n",
    "plot.new()\n",
    "\n",
    "# set parameters for layout, margins and main title relative text size\n",
    "par(mfrow=c(1,2), mar = rep(1,4), cex.main = 1.5)\n",
    "\n",
    "# inside data.table, for each type category (ham, spam), create a wordcloud and set a main title\n",
    "# note the use of curly braces \"{...}\" for multiple statements\n",
    "# \"by\" clause for aggregate/split operations\n",
    "sms_raw[, { wordcloud::wordcloud(text,\n",
    "         max.words = 40,\n",
    "         #scale = c(1, 1),\n",
    "         random.order = F,\n",
    "         colors = RColorBrewer::brewer.pal(8, \"Dark2\"))\n",
    "         title(main = type) }\n",
    ", by = type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can realize that \"spam\" cloud includes some words like \"free\", \"claim\", \"mobile\", \"prize\" that do not appear in the \"ham\" cloud\n",
    "\n",
    "within words common to both, \"call\" is more frequent in \"spam\" while \"now\", \"get\", \"just\", \"you\" and \"will\" are more frequent in \"ham\" cloud\n",
    "\n",
    "\"can\" appears in the \"ham\" cloud and not \"spam\" cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for more frequent words and creating indicator features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the data preparation process is to transform the sparse matrix into a data structure that can be used to train a Naive Bayes classifier.\n",
    "\n",
    "Currently, the sparse matrix includes over 6,500 features; this is a feature for every word that appears in at least one SMS message. It's unlikely that all of these are useful for classification.\n",
    "\n",
    "To reduce the number of features, we will eliminate any word that appear in less than five SMS messages, or in less than about 0.1 percent of the records in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_freq_words <- tm::findFreqTerms(sms_dtm_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1139 words appearing in at least 5 messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to filter our DTM to include only the terms appearing in a specified\n",
    "vector.\n",
    "\n",
    "As done earlier, we'll use the data frame style [row, col] operations to\n",
    "request specific portions of the DTM, noting that the columns are named after the\n",
    "words the DTM contains. We can take advantage of this to limit the DTM to specific\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]\n",
    "\n",
    "sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lapply(list(train = sms_dtm_train, freq_train = sms_dtm_freq_train), dim)\n",
    "lapply(list(test = sms_dtm_test, freq_test = sms_dtm_freq_test), dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how number of columns - denoting terms - shrank after the filtering. Row counts are preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier is typically trained on data with categorical features.\n",
    "\n",
    "This poses a problem, since the cells in the sparse matrix are numeric and measure the number of times a word appears in a message.\n",
    "\n",
    "We need to change this to a categorical variable that simply indicates yes or no depending on whether the word appears at all.\n",
    "\n",
    "We first create a custom function and then apply it on the column margin of the dtm's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_counts <- function(x)\n",
    "{\n",
    "    x <- ifelse(x > 0, \"Yes\", \"No\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)\n",
    "\n",
    "sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be two character type matrixes, each with cells indicating \"Yes\" or \"No\" for whether the word represented by the column appears at any point in the message represented by the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_train[1:10, 1:10]\n",
    "sms_test[1:10, 1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes algorithm will use the presence or absence of words to estimate the probability that a\n",
    "given SMS message is spam.\n",
    "\n",
    "To build our model on the sms_train matrix, we'll use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_classifier <- e1071::naiveBayes(sms_train, sms_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sms_classifier object now contains a naiveBayes classifier object that can be\n",
    "used to make predictions.\n",
    "\n",
    "See the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities appear inside the \"tables\" item of the list\n",
    "\n",
    "Now let's view and interpret the tables for selected terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_terms_spam <- c(\"free\", \"claim\", \"mobil\", \"prize\", \"call\")\n",
    "\n",
    "selected_terms_ham <- c(\"now\", \"get\", \"just\", \"you\", \"will\", \"can\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_classifier$table[selected_terms_spam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a selected term, the table should be read as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termm <- \"free\"\n",
    "table1 <- sms_classifier$table[[termm]]\n",
    "table1\n",
    "\n",
    "rows1 <- toupper(rownames(table1))\n",
    "cols1 <- toupper(c(\"does not appear\", \"appears\"))\n",
    "\n",
    "outer(1:2,\n",
    "      1:2,\n",
    "      Vectorize(function(x, y) sprintf(\"Given that the sms is a %s, the prob. that the term \\\"%s\\\" %s is %.2f\",\n",
    "                                      rows1[x], toupper(termm), cols1[y], table1[x,y]))) %>%\n",
    "    as.vector() %>%\n",
    "    data.frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How this info as it is, is not so useful. We try to figure out whether the message is a ham or spam based on the knowledge of whether the term appears.\n",
    "\n",
    "So we should process this info.\n",
    "\n",
    "Now a little bit information on how Naive Bayes algorithm works:\n",
    "\n",
    "Bayes theorem says: (from wikipedia)\n",
    "\n",
    "$${\\displaystyle p(C_{k}\\mid \\mathbf {x} )={\\frac {p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}{p(\\mathbf {x} )}}\\,}$$\n",
    "\n",
    "In plain English, using Bayesian probability terminology, the above equation can be written as\n",
    "\n",
    "$${\\displaystyle {\\mbox{posterior}}={\\frac {{\\mbox{prior}}\\times {\\mbox{likelihood}}}{\\mbox{evidence}}}\\,}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example:\n",
    "\n",
    "$${\\displaystyle {\\mbox{p(spam | free) }}={\\frac {{\\mbox{p(spam)}}\\times {\\mbox{p(free | spam)}}}{\\mbox{p(free)}}}\\,}$$\n",
    "\n",
    "So:\n",
    "- We start with \"the probability that FREE appears given the message is a SPAM\" (likelihood)\n",
    "- Multiply it with \"the probability of SPAM\" (prior)\n",
    "- Divide with \"the probability of FREE\" (evidence)\n",
    "\n",
    "\"evidence\" is simply the sum of p(spam) * ( p(free | spam) + p(free | ham) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sms classifier list holds the \"a priori\" counts of ham/spam, we convert them to proportions or probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop <- prop.table(sms_classifier$apriori)\n",
    "prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply the \"a priori\" probabilities with likelihoods (not matrix multiplication) to arrive at likelihoods of ham and spam given No/Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior1 <- as.numeric(prop) * table1\n",
    "posterior1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last we have to scale each likelihood column with their respective sums to arrive at the posterior probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior2 <- prop.table(posterior1, 2)\n",
    "posterior2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We interpret the posterior probabilities as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer(1:2,\n",
    "      1:2,\n",
    "      Vectorize(function(x, y) sprintf(\"Given that the term \\\"%s\\\" %s, the prob. that the sms is a %s is %.2f\",\n",
    "                                      toupper(termm), cols1[y], rows1[x], posterior2[x,y] ))) %>%\n",
    "    as.vector() %>%\n",
    "    data.frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are 74% confident that an sms with \"free\" is a spam and 89% confident that an sms without \"free\" is a ham "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplifying assumption about Naive Bayes is that, the probabilities of terms are independent of each other. So we can easily calculate posterior probabilities of joint conditions as:\n",
    "\n",
    "is:\n",
    "\n",
    "${\\displaystyle p(C_{k}\\mid x_{1},\\dots ,x_{n})={\\frac {1}{Z}}p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})}$\n",
    "\n",
    "where the evidence ${\\displaystyle Z=p(\\mathbf {x} )=\\sum _{k}p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}$ is a scaling factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remember the selected words likely to signal spams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_spam <- sms_classifier$table[c(\"free\", \"mobil\", \"call\")]\n",
    "table_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's collect only the second \"YES\" columns from each list item into a matrix elegantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam <- do.call(cbind, lapply(table_spam, \"[\", 1:2, 2))\n",
    "mat_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And calculate the joint independent likelihoods of each row before multiplying with \"a priori\" probs of ham or spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam2 <- apply(mat_spam, 1, prod)\n",
    "mat_spam2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pipe, we can do both in a single combined step.\n",
    "Note that we format the output to 6 digits for better display with sprintf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam2 <- do.call(cbind, lapply(table_spam, \"[\", 1:2, 2)) %>%\n",
    "    apply(1,prod)\n",
    "\n",
    "# we format the numbers as 6 digits and preserve the names in to vector\n",
    "sprintf(\"%.6f\", mat_spam2) %>%\n",
    "    purrr::set_names(names(mat_spam2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the \"a priori\" probabilities of ham and spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mat <- as.numeric(prop) * mat_spam2\n",
    "\n",
    "sprintf(\"%.6f\", posterior_mat) %>%\n",
    "    purrr::set_names(names(posterior_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And scale with likelihoods of Yes/No evidence:\n",
    "\n",
    "Note that we now further format the posterior probabilities:\n",
    "\n",
    "* To percentage\n",
    "* Keeping the original names\n",
    "* And with a right aligned table format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mat2 <- prop.table(posterior_mat)\n",
    "\n",
    "posterior_mat2 %>%\n",
    "    scales::percent(accuracy = 0.0001, trim = F) %>%\n",
    "    purrr::set_names(names(posterior_mat2)) %>%\n",
    "    knitr::kable(col.names = \"posterior\", align = \"r\") %>%\n",
    "    as.character() %>%\n",
    "    IRdisplay::display_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when \"free\", \"mobil\" and \"call\" terms appear together in an sms,\n",
    "\n",
    "with 99.9% prob. it is a spam,\n",
    "with only 0.08% prob. it is a ham!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Calculating posterior probabilities for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the prior likelihoods of all terms to calculate the posterior probabilities of the ham/spam classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam_all_ham <- do.call(cbind, lapply(sms_classifier$table, \"[\", 1, 1:2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam_all_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam_all_spam <- do.call(cbind, lapply(sms_classifier$table, \"[\", 2, 1:2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam_all_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the joint model, we have to multiply all likelihoods in a row. However zero values will distort the model, since multiplication with zero will yield zero (we'll see more on this below in the \"Laplace estimator\" section\n",
    "\n",
    "We will replace values below a certain threshold with that threshold value. We arbitrarily choose thresh = 0.001, since this is the default value that R's NaiveBayes method in predict() functon  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh <- 0.001\n",
    "mat_spam_all_ham[mat_spam_all_ham < thresh] <- thresh\n",
    "mat_spam_all_spam[mat_spam_all_spam < thresh] <- thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam_all_ham\n",
    "mat_spam_all_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dtm for test data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we'll calculate the joint \"ham\" and \"spam\" likelihoods (and then posterior probs) of all terms that appear or do not appear in this document: \n",
    "\n",
    "For each of ham/spam likelihoods, if the term does not appear, the likelihood in the \"No\" row, if the term appears, the likelihood in the \"Yes\" row will be taken and multiplied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termsn <- ncol(mat_spam_all_ham)\n",
    "sprintf(\"Number of terms is %s\", termsn)\n",
    "\n",
    "# This will give posterior probabilities of ham and spam for each document:\n",
    "# Note that in the second (third) row, \"ham\" (\"spam\") likelihoods from respective No/Yes rows of each term column of are subsetted\n",
    "posterior <- t(apply(sms_test, 1, function(x)\n",
    "    c(prod(mat_spam_all_ham[cbind((x == \"Yes\") + 1, 1:termsn)]),\n",
    "      prod(mat_spam_all_spam[cbind((x == \"Yes\") + 1, 1:termsn)])))) %>%  \n",
    "    prop.table(margin = 1)\n",
    "                     \n",
    "# this gives \"ham and spam\"\n",
    "names_hs <- names(posterior_mat2)\n",
    "sprintf(\"Column titles are %s\", paste(names_hs, collapse = \" and \"))\n",
    "\n",
    "# this will determine whether ham or spam posterior probability is higher\n",
    "# and hence classify the document accordingly:\n",
    "sms_test_pred_manual <- apply(posterior, 1, function(x) names_hs[which.max(x)])\n",
    "\n",
    "# format the posterior probabilities with appropriate percent decimals, column and row names:\n",
    "sms_test_pred_manual_percent <- posterior %>%\n",
    "    apply(2,scales::percent, accuracy = 0.0001, trim = F) %>%\n",
    "    magrittr::set_colnames(names_hs) %>%\n",
    "    magrittr::set_rownames(sms_test_pred_manual)\n",
    "\n",
    "sms_test_pred_manual_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with predict() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually do not manually go through the steps above to interpret the model:\n",
    "\n",
    "We can easily apply the model on the test set to calculate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_test_pred <- predict(sms_classifier, sms_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sms_test_pred is a factor vector of levels \"ham\" and \"spam\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as sms_test_labels is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the predict function we can also get the ham/spam posterior probabilities for each document with the \"raw\" option for type argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_test_pred_raw <- predict(sms_classifier, sms_test, type = \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sms_test_pred_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print this matrix nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham and spam\n",
    "names_hs <- names(posterior_mat2)\n",
    "\n",
    "# set percent format, rownames and column names\n",
    "sms_test_pred_raw_percent <- sms_test_pred_raw %>%     \n",
    "    apply(2,scales::percent, accuracy = 0.0001, trim = F) %>%\n",
    "    magrittr::set_colnames(names_hs) %>%\n",
    "    magrittr::set_rownames(apply(sms_test_pred_raw, 1, function(x) names_hs[which.max(x)]))\n",
    "\n",
    "sms_test_pred_raw_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the ham/spam classifications we did manually and through the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(as.factor(sms_test_pred_manual), sms_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly they are parallel, but there may be some classification differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_predict_match <- sms_test_pred_manual == as.character(sms_test_pred)\n",
    "\n",
    "sprintf(\"From a total of %s documents, in %s ones both our manual and predict function classifications are the same\",\n",
    "total_doc <- length(sms_test_pred_manual),\n",
    "cor_match <- sum(manual_predict_match))\n",
    "\n",
    "sprintf(\"So classifications of %s documents differ between our manual method and predict function\",\n",
    "        total_doc - cor_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick those documents where classifications of predict() function is different from our manual calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mismatches from manual posterior probs.\n",
    "m1 <- sms_test_pred_manual_percent[!manual_predict_match,]\n",
    "# mismatches from predict() function\n",
    "m2 <- sms_test_pred_raw_percent[!manual_predict_match,]\n",
    "\n",
    "# display both matrices side by side\n",
    "knitr::kable(list(list(m1, caption = data.frame(calculated_by = \"manual\")),\n",
    "                  list(m2, caption = data.frame(calculated_by = \"predict\")))) %>%\n",
    "    as.character() %>%\n",
    "    IRdisplay::display_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that in none of those mismatches, there is any highly dominant posterior probability such as > 99% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the differences are mostly from the accuracy limits of numeric values in R: Base R can only handle up to 22 decimal digits. After that, accuracy is lost\n",
    "\n",
    "Multiplying +1000 small numbers can lose accuracy. This may happen in our manual calculation. Libraries like Rmpfr can handle much detailed accuracy.\n",
    "\n",
    "predict() function is probably designed to handle such possible accuracy losses. So it is better to use the built in predict() function to predict the labels of the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will form a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossTable visualizes how much the test labels are correctly classified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nb <- gmodels::CrossTable(sms_test_pred,\n",
    "                             sms_test_labels,\n",
    "                             prop.chisq = F,\n",
    "                             prop.t = F,\n",
    "                             prop.r = F,\n",
    "                             dnn = c('predicted', 'actual'))\n",
    "\n",
    "ct_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's automatically report findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintf(\"Out of a total of %s sms's:\n",
    "- %s sms's are correctly classified as either ham or spam (%s),\n",
    "- while %s sms's are misclassified (%s)\",\n",
    "        all <- ct_nb$t %>% sum(),\n",
    "        cor <- ct_nb$t %>% diag() %>% sum(),\n",
    "        (cor / all) %>% scales::percent(accuracy = 0.01),\n",
    "        fal <- all - cor,\n",
    "        (fal / all) %>% scales::percent(accuracy = 0.01)\n",
    "       ) %>% cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get those terms that never occur in spams in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(sms_classifier$tables, \"[\", 2, 2) %>%\n",
    "    magrittr::extract(.==0) %>%\n",
    "    names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a word here which we think, might signal a spam message, such as \"present\". And let's repeat the steps above including \"present\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_spam <- sms_classifier$table[c(\"free\", \"mobil\", \"call\", \"present\")]\n",
    "table_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam <- do.call(cbind, lapply(table_spam, \"[\", 1:2, 2))\n",
    "mat_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam2 <- do.call(cbind, lapply(table_spam, \"[\", 1:2, 2)) %>%\n",
    "    apply(1,prod)\n",
    "\n",
    "# we format the numbers as 6 digits and preserve the names in to vector\n",
    "sprintf(\"%.10f\", mat_spam2) %>%\n",
    "    purrr::set_names(names(mat_spam2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_spam2 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that, with the \"zero\" occurrence word \"present\", our model tend to misclassify mails that contain \"free\", \"mobil\", \"call\" and \"present\" as a ham with full confidence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplication with 0 yields 0. So we have to correct with this using the laplace estimator by adding a very small number to 0 likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_test_pred2 <- predict(sms_classifier2, sms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct_nb2 <- gmodels::CrossTable(sms_test_pred2,\n",
    "           sms_test_labels,\n",
    "           prop.chisq = F,\n",
    "           prop.t = F,\n",
    "           prop.r = F,\n",
    "           dnn = c('predicted', 'actual'))\n",
    "\n",
    "ct_nb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintf(\"Out of a total of %s sms's:\n",
    "- %s sms's are correctly classified as either ham or spam (%s),\n",
    "- while %s sms's are misclassified (%s)\",\n",
    "        all2 <- ct_nb2$t %>% sum(),\n",
    "        cor2 <- ct_nb2$t %>% diag() %>% sum(),\n",
    "        (cor2 / all2) %>% scales::percent(accuracy = 0.01),\n",
    "        fal2 <- all2 - cor2,\n",
    "        (fal2 / all2) %>% scales::percent(accuracy = 0.01)\n",
    "       ) %>% cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf <- c(\"Worse\", \"Same\", \"Better\")\n",
    "\n",
    "sprintf(\"%s performance of new vs. old:\n",
    "(%s vs. %s correct,\n",
    "%s vs. %s misclassified)\",\n",
    "    perf[sign(cor2 - cor) + 2],\n",
    "    cor2, cor, fal2, fal) %>% cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save.image(sessionfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
